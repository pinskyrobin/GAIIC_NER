{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一、商品标题实体识别竞赛\n",
    "比赛地址：[https://www.heywhale.com/org/gaiic2022/competition/area/620b34ed28270b0017b823ad/content/2](https://www.heywhale.com/org/gaiic2022/competition/area/620b34ed28270b0017b823ad/content/2)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2bae422f9a92415d8d5f55c8998f19f2b263643a073e4daba0642c2277066ce1)\n",
    "\n",
    "### 1.赛题背景\n",
    "京东商品标题包含了商品的大量关键信息，商品标题实体识别是NLP应用中的一项核心基础任务，能为多种下游场景所复用，从标题文本中准确抽取出商品相关实体能够提升检索、推荐等业务场景下的用户体验和平台效率。本赛题要求选手使用模型抽取出商品标题文本中的实体。\n",
    "与传统的实体抽取不同，京东商品标题文本的实体密度高、实体粒度细，赛题具有特色性。\n",
    " \n",
    "\n",
    "###  2.比赛数据\n",
    "- 1.本赛题数据来源于特定类目的京东商品标题短文本，分为有标注样本和无标注样本，供选手选择使用。\n",
    "\n",
    "- 2.数据格式：训练集数据每一行第一列为一个字符或空格（汉字、英文字母、数字、标点符号、特殊符号、空格），第二列为BIO形式的标签，两列以空格分隔。\n",
    "\n",
    "- 3.两条标注样本之间以空行为分割。\n",
    "\n",
    "- 4.训练集：有标注训练样本：4万条左右（包括验证集，不再单独提供验证集，由选手自己切分；总量根据baseline模型效果可能会稍作调整）；无标注样本：100万条。\n",
    "初赛A榜测试集：1万条（与训练样本格式相同，差异仅在于无标注）<br>\n",
    "初赛B榜测试集：1万条（与训练样本格式相同，差异仅在于无标注）<br>\n",
    "复赛测试集：1万条（与训练样本格式相同，差异仅在于无标注）<br>\n",
    "决赛测试集：1万条（与训练样本格式相同，差异仅在于无标注）\n",
    "\n",
    "- 5.标注样本示例\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/97c9f7d721544397941ffd6d2d0ec9200872c5a335814f179de7a4db1907cdce)\n",
    "\n",
    "👉[点击下载样例](https://www.heywhale.com/u/c029ee)\n",
    "\n",
    "- 6.实体说明\n",
    "\n",
    "实体共有52种类型，均已经过脱敏处理，用数字代号1至54表示（不包含27和45）；其中“O”为非实体。标签中“B”代表一个实体的开始，“I”代表一个实体的中间或者结尾。“-”后的数字代号表示该字符的实体类型。\n",
    "\n",
    "值得注意的是实体不仅仅与实体词有关，而且与当前标题所售卖商品有关。举例说明，一个售卖产品为手机壳的商品标题中出现的“iPhone13”与售卖产品为手机的商品标题中出现的“iPhone13”为不同的实体标签。\n",
    "\n",
    " \n",
    "\n",
    "### 3.数据下载\n",
    "\n",
    "\n",
    "| 数据名称 | 下载链接| Column 3 |\n",
    "| -------- | -------- | -------- |\n",
    "| 数据样例     | 初赛训练集数据样例\t  |   👉🏻 [点击下载](https://open-cdn.kesci.com/admin/r7uws515n8/train_500.txt)   |\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  二、RNN命名实体识别概念\n",
    "在2017年之前，工业界和学术界对NLP文本处理依赖于序列模型[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>图1：RNN示意图</center></br>\n",
    "\n",
    "<br>\n",
    "\n",
    "近年来随着深度学习的发展，模型参数的数量飞速增长。为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分NLP任务来说，构建大规模的标注数据集非常困难（成本过高），特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。为了利用这些数据，我们可以先从其中学习到一个好的表示，再将这些表示应用到其他任务中。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 在NLP任务上取得了很好的表现。\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的发展，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>图2：预训练模型一览，图片来源于：https://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                       \n",
    "                                                                       \n",
    "在此，使用ERNIE预训练模型完成序列标注任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 三、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可见训练集由已标注数据和未标注数据组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### 1.数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!awk '{print NR}' data/train.txt | tail -n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.数据格式调整\n",
    "PaddleNLP格式为一条记录一条记录的样式，在此需要对原始数据进行格式调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "\r\n",
    "\r\n",
    "def format_data(source_filename, target_filename):\r\n",
    "    datalist = []\r\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    words = ''\r\n",
    "    labels = ''\r\n",
    "    word=''\r\n",
    "    label=''\r\n",
    "    flag = 0\r\n",
    "    cnt = 0\r\n",
    "    for line in lines:\r\n",
    "        if line == '\\n':\r\n",
    "            item = words + '\\t' + labels + '\\n'\r\n",
    "            datalist.append(item)\r\n",
    "            words = ''\r\n",
    "            labels = ''\r\n",
    "            flag = 0\r\n",
    "            continue\r\n",
    "        elif line == '  O\\n':\r\n",
    "            word = ' '\r\n",
    "            label = 'O'\r\n",
    "        elif '  I-'in line:\r\n",
    "            word = ' '\r\n",
    "            label = line.strip('\\n').strip(' ')\r\n",
    "        else:\r\n",
    "            word, label = line.strip('\\n').split(' ')\r\n",
    "\r\n",
    "        # 使用\\002是为了区分空格\r\n",
    "        if flag == 1:\r\n",
    "            words = words + '\\002' + word\r\n",
    "            labels = labels + '\\002' + label\r\n",
    "        else:\r\n",
    "            words = words + word\r\n",
    "            labels = labels + label\r\n",
    "            flag = 1\r\n",
    "    # 添加最后一行数据\r\n",
    "    # item = words + '\\t' + labels + '\\n'\r\n",
    "    # datalist.append(item)\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        lines = f.writelines(datalist)\r\n",
    "    print(f'{source_filename}文件格式转换完毕，保存为{target_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_data('data/train.txt', 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tail -n3 train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!awk '{print NR}' train.txt | tail -n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.数据集划分\n",
    "按8:2进行分割，`-n`后面的数目具体由数据集的大小决定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  训练集\r\n",
    "!head -n65000 train.txt >train_converted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 后 测试集\r\n",
    "!tail -n15000 train.txt >val_converted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.PaddleNLP环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -U pip --user >log.log\r\n",
    "!pip install --upgrade paddlenlp >log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\r\n",
    "\r\n",
    "import paddle\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.加载自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            for line in fp.readlines():\r\n",
    "                words, labels = line.strip('\\n').split('\\t')\r\n",
    "                words = words.split('\\002')\r\n",
    "                labels = labels.split('\\002')\r\n",
    "                yield words, labels\r\n",
    "\r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建dataset\r\n",
    "train_ds, dev_ds = load_dataset(datafiles=(\r\n",
    "        './train_converted.txt', './val_converted.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\r\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.label标签表构建\n",
    "每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成标签\r\n",
    "def gernate_dic(source_filename1,  target_filename):\r\n",
    "    data_list=[]\r\n",
    "\r\n",
    "    with open(source_filename1, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "\r\n",
    "    for line in lines:\r\n",
    "        if line == '\\n':\r\n",
    "            continue\r\n",
    "        elif line == '  O\\n':\r\n",
    "            dic = 'O'\r\n",
    "        elif '  I-'in line:\r\n",
    "            dic = line.strip('\\n').strip(' ')\r\n",
    "        else:\r\n",
    "            dic = line.strip('\\n').split(' ')[-1]\r\n",
    "        if dic+'\\n' not in data_list:\r\n",
    "            data_list.append(dic+'\\n')    \r\n",
    "\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        lines=f.writelines(data_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成dic\r\n",
    "gernate_dic('data/train.txt', 'mytag.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看生成的dic文件\r\n",
    "!cat mytag.dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 7.数据处理\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "![https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png](https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png)\n",
    "\n",
    "![https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png](https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_vocab = load_dict('mytag.dic')\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "\r\n",
    "train_ds.map(trans_func)\r\n",
    "dev_ds.map(trans_func)\r\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据读入**\n",
    "\n",
    "使用paddle.io.DataLoader接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): fn(samples)\r\n",
    "\r\n",
    "train_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_size=256,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)\r\n",
    "dev_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_size=64,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 四、PaddleNLP一键加载预训练模型\n",
    "\n",
    "### 1.加载预训练模型\n",
    "\n",
    "快递单信息抽取本质是一个序列标注任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务文本分类Fine-tune网络。以下教程以ERNIE为预训练模型完成序列标注任务。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`一行代码即可加载预训练模型ERNIE用于序列标注任务的fine-tune网络。其在ERNIE模型后拼接上一个全连接网络进行分类。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`方法只需指定想要使用的模型名称和文本分类的类别数即可完成定义模型网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model netword and its loss\r\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。您可以使用PaddleNLP提供的模型，完成文本分类、序列标注、问答等任务。同时我们提供了众多预训练模型的参数权重供用户使用，其中包含了二十多种中文语言模型的预训练权重。中文的预训练模型有`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`等。\n",
    "\n",
    "更多预训练模型参考：[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)。\n",
    "\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考：[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)。\n",
    "\n",
    "### 2.超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\r\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=1.28e-4, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 五、模型训练与评估\n",
    "\n",
    "### 1.训练模型\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step = 0\r\n",
    "for epoch in range(10):\r\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        step += 1\r\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\r\n",
    "    evaluate(model, metric, dev_loader)\r\n",
    "\r\n",
    "    paddle.save(model.state_dict(),\r\n",
    "                './checkpoint/model_%d.pdparams' % step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir ernie_result\r\n",
    "model.save_pretrained('./ernie_result')\r\n",
    "tokenizer.save_pretrained('./ernie_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 六、预测\n",
    "\n",
    "###  1.重启notebook，释放显存\n",
    "\n",
    "重启notebook，释放显存，开始训练。\n",
    "\n",
    "训练保存好的训练，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import DataLoader\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from utils import convert_example, evaluate, predict, load_dict\r\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.导入预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n20 data/sample_per_line_preliminary_B.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.定义test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            # next(fp)  # 没有header，不用Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                words = line.strip('\\n')\r\n",
    "                words=[ch for ch in words]\r\n",
    "                labels=['O' for x in range(0,len(words))]\r\n",
    "\r\n",
    "                yield words, labels\r\n",
    "                # yield words\r\n",
    "\r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ds = load_dataset(datafiles=('./data/sample_per_line_preliminary_B.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\r\n",
    "    print(test_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.加载最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_vocab = load_dict('./mytag.dic')\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "test_ds.map(trans_func)\r\n",
    "print (test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = 1\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): fn(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_size=30,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_predict(model, data_loader, ds, label_vocab):\r\n",
    "    pred_list = []\r\n",
    "    len_list = []\r\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\r\n",
    "        logits = model(input_ids, seg_ids)\r\n",
    "        # print(len(logits[0]))\r\n",
    "        pred = paddle.argmax(logits, axis=-1)\r\n",
    "        pred_list.append(pred.numpy())\r\n",
    "        len_list.append(lens.numpy())\r\n",
    "    preds ,tags= parse_decodes(ds, pred_list, len_list, label_vocab)\r\n",
    "    return preds, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 选择目标checkpoint\r\n",
    "model_dict = paddle.load('checkpoint/model_2540.pdparams')\r\n",
    "model.set_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.预测并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\r\n",
    "preds, tags = my_predict(model, test_loader, test_ds, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tags[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"ernie_results.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "    fout.write(\"\\n\".join(preds))\r\n",
    "# Print some examples\r\n",
    "print(\r\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "    % file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head ernie_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 调整数据格式\r\n",
    "wp = open('data/sample_per_line_preliminary_B.txt', 'r', encoding='utf-8')\r\n",
    "lp = open('./ernie_results.txt', 'r', encoding='utf-8')\r\n",
    "with open('result.txt', 'w', encoding='utf-8') as f:\r\n",
    "    word_line = wp.readlines()\r\n",
    "    label_line = lp.readlines()\r\n",
    "    print('word_line:{},label_line {}'.format(len(word_line), len(label_line)))\r\n",
    "    for i in range(len(word_line)):\r\n",
    "        labels = label_line[i].strip('\\n').strip(' ').split(' ')\r\n",
    "        words = word_line[i].strip('\\n')\r\n",
    "        print('i:{} labels:{},words {}'.format(i, len(labels), len(words)))\r\n",
    "        for j in range(len(labels)):\r\n",
    "            f.write(words[j] + ' ' + labels[j] + '\\n')\r\n",
    "        f.write('\\n')\r\n",
    "wp.close()\r\n",
    "lp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
