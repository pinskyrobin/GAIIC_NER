{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ä¸€ã€å•†å“æ ‡é¢˜å®ä½“è¯†åˆ«ç«èµ›\n",
    "æ¯”èµ›åœ°å€ï¼š[https://www.heywhale.com/org/gaiic2022/competition/area/620b34ed28270b0017b823ad/content/2](https://www.heywhale.com/org/gaiic2022/competition/area/620b34ed28270b0017b823ad/content/2)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2bae422f9a92415d8d5f55c8998f19f2b263643a073e4daba0642c2277066ce1)\n",
    "\n",
    "### 1.èµ›é¢˜èƒŒæ™¯\n",
    "äº¬ä¸œå•†å“æ ‡é¢˜åŒ…å«äº†å•†å“çš„å¤§é‡å…³é”®ä¿¡æ¯ï¼Œå•†å“æ ‡é¢˜å®ä½“è¯†åˆ«æ˜¯NLPåº”ç”¨ä¸­çš„ä¸€é¡¹æ ¸å¿ƒåŸºç¡€ä»»åŠ¡ï¼Œèƒ½ä¸ºå¤šç§ä¸‹æ¸¸åœºæ™¯æ‰€å¤ç”¨ï¼Œä»æ ‡é¢˜æ–‡æœ¬ä¸­å‡†ç¡®æŠ½å–å‡ºå•†å“ç›¸å…³å®ä½“èƒ½å¤Ÿæå‡æ£€ç´¢ã€æ¨èç­‰ä¸šåŠ¡åœºæ™¯ä¸‹çš„ç”¨æˆ·ä½“éªŒå’Œå¹³å°æ•ˆç‡ã€‚æœ¬èµ›é¢˜è¦æ±‚é€‰æ‰‹ä½¿ç”¨æ¨¡å‹æŠ½å–å‡ºå•†å“æ ‡é¢˜æ–‡æœ¬ä¸­çš„å®ä½“ã€‚\n",
    "ä¸ä¼ ç»Ÿçš„å®ä½“æŠ½å–ä¸åŒï¼Œäº¬ä¸œå•†å“æ ‡é¢˜æ–‡æœ¬çš„å®ä½“å¯†åº¦é«˜ã€å®ä½“ç²’åº¦ç»†ï¼Œèµ›é¢˜å…·æœ‰ç‰¹è‰²æ€§ã€‚\n",
    " \n",
    "\n",
    "###  2.æ¯”èµ›æ•°æ®\n",
    "- 1.æœ¬èµ›é¢˜æ•°æ®æ¥æºäºç‰¹å®šç±»ç›®çš„äº¬ä¸œå•†å“æ ‡é¢˜çŸ­æ–‡æœ¬ï¼Œåˆ†ä¸ºæœ‰æ ‡æ³¨æ ·æœ¬å’Œæ— æ ‡æ³¨æ ·æœ¬ï¼Œä¾›é€‰æ‰‹é€‰æ‹©ä½¿ç”¨ã€‚\n",
    "\n",
    "- 2.æ•°æ®æ ¼å¼ï¼šè®­ç»ƒé›†æ•°æ®æ¯ä¸€è¡Œç¬¬ä¸€åˆ—ä¸ºä¸€ä¸ªå­—ç¬¦æˆ–ç©ºæ ¼ï¼ˆæ±‰å­—ã€è‹±æ–‡å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šç¬¦å·ã€ç©ºæ ¼ï¼‰ï¼Œç¬¬äºŒåˆ—ä¸ºBIOå½¢å¼çš„æ ‡ç­¾ï¼Œä¸¤åˆ—ä»¥ç©ºæ ¼åˆ†éš”ã€‚\n",
    "\n",
    "- 3.ä¸¤æ¡æ ‡æ³¨æ ·æœ¬ä¹‹é—´ä»¥ç©ºè¡Œä¸ºåˆ†å‰²ã€‚\n",
    "\n",
    "- 4.è®­ç»ƒé›†ï¼šæœ‰æ ‡æ³¨è®­ç»ƒæ ·æœ¬ï¼š4ä¸‡æ¡å·¦å³ï¼ˆåŒ…æ‹¬éªŒè¯é›†ï¼Œä¸å†å•ç‹¬æä¾›éªŒè¯é›†ï¼Œç”±é€‰æ‰‹è‡ªå·±åˆ‡åˆ†ï¼›æ€»é‡æ ¹æ®baselineæ¨¡å‹æ•ˆæœå¯èƒ½ä¼šç¨ä½œè°ƒæ•´ï¼‰ï¼›æ— æ ‡æ³¨æ ·æœ¬ï¼š100ä¸‡æ¡ã€‚\n",
    "åˆèµ›Aæ¦œæµ‹è¯•é›†ï¼š1ä¸‡æ¡ï¼ˆä¸è®­ç»ƒæ ·æœ¬æ ¼å¼ç›¸åŒï¼Œå·®å¼‚ä»…åœ¨äºæ— æ ‡æ³¨ï¼‰<br>\n",
    "åˆèµ›Bæ¦œæµ‹è¯•é›†ï¼š1ä¸‡æ¡ï¼ˆä¸è®­ç»ƒæ ·æœ¬æ ¼å¼ç›¸åŒï¼Œå·®å¼‚ä»…åœ¨äºæ— æ ‡æ³¨ï¼‰<br>\n",
    "å¤èµ›æµ‹è¯•é›†ï¼š1ä¸‡æ¡ï¼ˆä¸è®­ç»ƒæ ·æœ¬æ ¼å¼ç›¸åŒï¼Œå·®å¼‚ä»…åœ¨äºæ— æ ‡æ³¨ï¼‰<br>\n",
    "å†³èµ›æµ‹è¯•é›†ï¼š1ä¸‡æ¡ï¼ˆä¸è®­ç»ƒæ ·æœ¬æ ¼å¼ç›¸åŒï¼Œå·®å¼‚ä»…åœ¨äºæ— æ ‡æ³¨ï¼‰\n",
    "\n",
    "- 5.æ ‡æ³¨æ ·æœ¬ç¤ºä¾‹\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/97c9f7d721544397941ffd6d2d0ec9200872c5a335814f179de7a4db1907cdce)\n",
    "\n",
    "ğŸ‘‰[ç‚¹å‡»ä¸‹è½½æ ·ä¾‹](https://www.heywhale.com/u/c029ee)\n",
    "\n",
    "- 6.å®ä½“è¯´æ˜\n",
    "\n",
    "å®ä½“å…±æœ‰52ç§ç±»å‹ï¼Œå‡å·²ç»è¿‡è„±æ•å¤„ç†ï¼Œç”¨æ•°å­—ä»£å·1è‡³54è¡¨ç¤ºï¼ˆä¸åŒ…å«27å’Œ45ï¼‰ï¼›å…¶ä¸­â€œOâ€ä¸ºéå®ä½“ã€‚æ ‡ç­¾ä¸­â€œBâ€ä»£è¡¨ä¸€ä¸ªå®ä½“çš„å¼€å§‹ï¼Œâ€œIâ€ä»£è¡¨ä¸€ä¸ªå®ä½“çš„ä¸­é—´æˆ–è€…ç»“å°¾ã€‚â€œ-â€åçš„æ•°å­—ä»£å·è¡¨ç¤ºè¯¥å­—ç¬¦çš„å®ä½“ç±»å‹ã€‚\n",
    "\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯å®ä½“ä¸ä»…ä»…ä¸å®ä½“è¯æœ‰å…³ï¼Œè€Œä¸”ä¸å½“å‰æ ‡é¢˜æ‰€å”®å–å•†å“æœ‰å…³ã€‚ä¸¾ä¾‹è¯´æ˜ï¼Œä¸€ä¸ªå”®å–äº§å“ä¸ºæ‰‹æœºå£³çš„å•†å“æ ‡é¢˜ä¸­å‡ºç°çš„â€œiPhone13â€ä¸å”®å–äº§å“ä¸ºæ‰‹æœºçš„å•†å“æ ‡é¢˜ä¸­å‡ºç°çš„â€œiPhone13â€ä¸ºä¸åŒçš„å®ä½“æ ‡ç­¾ã€‚\n",
    "\n",
    " \n",
    "\n",
    "### 3.æ•°æ®ä¸‹è½½\n",
    "\n",
    "\n",
    "| æ•°æ®åç§° | ä¸‹è½½é“¾æ¥| Column 3 |\n",
    "| -------- | -------- | -------- |\n",
    "| æ•°æ®æ ·ä¾‹     | åˆèµ›è®­ç»ƒé›†æ•°æ®æ ·ä¾‹\t  |   ğŸ‘‰ğŸ» [ç‚¹å‡»ä¸‹è½½](https://open-cdn.kesci.com/admin/r7uws515n8/train_500.txt)   |\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  äºŒã€RNNå‘½åå®ä½“è¯†åˆ«æ¦‚å¿µ\n",
    "åœ¨2017å¹´ä¹‹å‰ï¼Œå·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œå¯¹NLPæ–‡æœ¬å¤„ç†ä¾èµ–äºåºåˆ—æ¨¡å‹[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>å›¾1ï¼šRNNç¤ºæ„å›¾</center></br>\n",
    "\n",
    "<br>\n",
    "\n",
    "è¿‘å¹´æ¥éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡é£é€Ÿå¢é•¿ã€‚ä¸ºäº†è®­ç»ƒè¿™äº›å‚æ•°ï¼Œéœ€è¦æ›´å¤§çš„æ•°æ®é›†æ¥é¿å…è¿‡æ‹Ÿåˆã€‚ç„¶è€Œï¼Œå¯¹äºå¤§éƒ¨åˆ†NLPä»»åŠ¡æ¥è¯´ï¼Œæ„å»ºå¤§è§„æ¨¡çš„æ ‡æ³¨æ•°æ®é›†éå¸¸å›°éš¾ï¼ˆæˆæœ¬è¿‡é«˜ï¼‰ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¥æ³•å’Œè¯­ä¹‰ç›¸å…³çš„ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§è§„æ¨¡çš„æœªæ ‡æ³¨è¯­æ–™åº“çš„æ„å»ºåˆ™ç›¸å¯¹å®¹æ˜“ã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆä»å…¶ä¸­å­¦ä¹ åˆ°ä¸€ä¸ªå¥½çš„è¡¨ç¤ºï¼Œå†å°†è¿™äº›è¡¨ç¤ºåº”ç”¨åˆ°å…¶ä»–ä»»åŠ¡ä¸­ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¤§è§„æ¨¡æœªæ ‡æ³¨è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPretrained Models, PTM) åœ¨NLPä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ã€‚\n",
    "\n",
    "è¿‘å¹´æ¥ï¼Œå¤§é‡çš„ç ”ç©¶è¡¨æ˜åŸºäºå¤§å‹è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPretrained Models, PTMï¼‰å¯ä»¥å­¦ä¹ é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œæœ‰åˆ©äºä¸‹æ¸¸NLPä»»åŠ¡ï¼ŒåŒæ—¶èƒ½å¤Ÿé¿å…ä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚éšç€è®¡ç®—èƒ½åŠ›çš„å‘å±•ï¼Œæ·±åº¦æ¨¡å‹çš„å‡ºç°ï¼ˆå³ Transformerï¼‰å’Œè®­ç»ƒæŠ€å·§çš„å¢å¼ºä½¿å¾— PTM ä¸æ–­å‘å±•ï¼Œç”±æµ…å˜æ·±ã€‚\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>å›¾2ï¼šé¢„è®­ç»ƒæ¨¡å‹ä¸€è§ˆï¼Œå›¾ç‰‡æ¥æºäºï¼šhttps://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                       \n",
    "                                                                       \n",
    "åœ¨æ­¤ï¼Œä½¿ç”¨ERNIEé¢„è®­ç»ƒæ¨¡å‹å®Œæˆåºåˆ—æ ‡æ³¨ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ä¸‰ã€æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "å¯è§è®­ç»ƒé›†ç”±å·²æ ‡æ³¨æ•°æ®å’Œæœªæ ‡æ³¨æ•°æ®ç»„æˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### 1.æ•°æ®æŸ¥çœ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!awk '{print NR}' data/train.txt | tail -n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.æ•°æ®æ ¼å¼è°ƒæ•´\n",
    "PaddleNLPæ ¼å¼ä¸ºä¸€æ¡è®°å½•ä¸€æ¡è®°å½•çš„æ ·å¼ï¼Œåœ¨æ­¤éœ€è¦å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ ¼å¼è°ƒæ•´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "\r\n",
    "\r\n",
    "def format_data(source_filename, target_filename):\r\n",
    "    datalist = []\r\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    words = ''\r\n",
    "    labels = ''\r\n",
    "    word=''\r\n",
    "    label=''\r\n",
    "    flag = 0\r\n",
    "    cnt = 0\r\n",
    "    for line in lines:\r\n",
    "        if line == '\\n':\r\n",
    "            item = words + '\\t' + labels + '\\n'\r\n",
    "            datalist.append(item)\r\n",
    "            words = ''\r\n",
    "            labels = ''\r\n",
    "            flag = 0\r\n",
    "            continue\r\n",
    "        elif line == '  O\\n':\r\n",
    "            word = ' '\r\n",
    "            label = 'O'\r\n",
    "        elif '  I-'in line:\r\n",
    "            word = ' '\r\n",
    "            label = line.strip('\\n').strip(' ')\r\n",
    "        else:\r\n",
    "            word, label = line.strip('\\n').split(' ')\r\n",
    "\r\n",
    "        # ä½¿ç”¨\\002æ˜¯ä¸ºäº†åŒºåˆ†ç©ºæ ¼\r\n",
    "        if flag == 1:\r\n",
    "            words = words + '\\002' + word\r\n",
    "            labels = labels + '\\002' + label\r\n",
    "        else:\r\n",
    "            words = words + word\r\n",
    "            labels = labels + label\r\n",
    "            flag = 1\r\n",
    "    # æ·»åŠ æœ€åä¸€è¡Œæ•°æ®\r\n",
    "    # item = words + '\\t' + labels + '\\n'\r\n",
    "    # datalist.append(item)\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        lines = f.writelines(datalist)\r\n",
    "    print(f'{source_filename}æ–‡ä»¶æ ¼å¼è½¬æ¢å®Œæ¯•ï¼Œä¿å­˜ä¸º{target_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_data('data/train.txt', 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tail -n3 train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!awk '{print NR}' train.txt | tail -n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.æ•°æ®é›†åˆ’åˆ†\n",
    "æŒ‰8:2è¿›è¡Œåˆ†å‰²ï¼Œ`-n`åé¢çš„æ•°ç›®å…·ä½“ç”±æ•°æ®é›†çš„å¤§å°å†³å®šï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  è®­ç»ƒé›†\r\n",
    "!head -n65000 train.txt >train_converted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# å æµ‹è¯•é›†\r\n",
    "!tail -n15000 train.txt >val_converted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.PaddleNLPç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -U pip --user >log.log\r\n",
    "!pip install --upgrade paddlenlp >log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\r\n",
    "\r\n",
    "import paddle\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            for line in fp.readlines():\r\n",
    "                words, labels = line.strip('\\n').split('\\t')\r\n",
    "                words = words.split('\\002')\r\n",
    "                labels = labels.split('\\002')\r\n",
    "                yield words, labels\r\n",
    "\r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# åˆ›å»ºdataset\r\n",
    "train_ds, dev_ds = load_dataset(datafiles=(\r\n",
    "        './train_converted.txt', './val_converted.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\r\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.labelæ ‡ç­¾è¡¨æ„å»º\n",
    "æ¯æ¡æ•°æ®åŒ…å«ä¸€å¥æ–‡æœ¬å’Œè¿™ä¸ªæ–‡æœ¬ä¸­æ¯ä¸ªæ±‰å­—ä»¥åŠæ•°å­—å¯¹åº”çš„labelæ ‡ç­¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ ‡ç­¾\r\n",
    "def gernate_dic(source_filename1,  target_filename):\r\n",
    "    data_list=[]\r\n",
    "\r\n",
    "    with open(source_filename1, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "\r\n",
    "    for line in lines:\r\n",
    "        if line == '\\n':\r\n",
    "            continue\r\n",
    "        elif line == '  O\\n':\r\n",
    "            dic = 'O'\r\n",
    "        elif '  I-'in line:\r\n",
    "            dic = line.strip('\\n').strip(' ')\r\n",
    "        else:\r\n",
    "            dic = line.strip('\\n').split(' ')[-1]\r\n",
    "        if dic+'\\n' not in data_list:\r\n",
    "            data_list.append(dic+'\\n')    \r\n",
    "\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        lines=f.writelines(data_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ç”Ÿæˆdic\r\n",
    "gernate_dic('data/train.txt', 'mytag.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ç”Ÿæˆçš„dicæ–‡ä»¶\r\n",
    "!cat mytag.dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 7.æ•°æ®å¤„ç†\n",
    "é¢„è®­ç»ƒæ¨¡å‹ERNIEå¯¹ä¸­æ–‡æ•°æ®çš„å¤„ç†æ˜¯ä»¥å­—ä¸ºå•ä½ã€‚PaddleNLPå¯¹äºå„ç§é¢„è®­ç»ƒæ¨¡å‹å·²ç»å†…ç½®äº†ç›¸åº”çš„tokenizerã€‚æŒ‡å®šæƒ³è¦ä½¿ç”¨çš„æ¨¡å‹åå­—å³å¯åŠ è½½å¯¹åº”çš„tokenizerã€‚\n",
    "\n",
    "tokenizerä½œç”¨ä¸ºå°†åŸå§‹è¾“å…¥æ–‡æœ¬è½¬åŒ–æˆæ¨¡å‹modelå¯ä»¥æ¥å—çš„è¾“å…¥æ•°æ®å½¢å¼ã€‚\n",
    "\n",
    "![https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png](https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png)\n",
    "\n",
    "![https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png](https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_vocab = load_dict('mytag.dic')\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "\r\n",
    "train_ds.map(trans_func)\r\n",
    "dev_ds.map(trans_func)\r\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**æ•°æ®è¯»å…¥**\n",
    "\n",
    "ä½¿ç”¨paddle.io.DataLoaderæ¥å£å¤šçº¿ç¨‹å¼‚æ­¥åŠ è½½æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): fn(samples)\r\n",
    "\r\n",
    "train_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_size=256,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)\r\n",
    "dev_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_size=64,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## å››ã€PaddleNLPä¸€é”®åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "### 1.åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "å¿«é€’å•ä¿¡æ¯æŠ½å–æœ¬è´¨æ˜¯ä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼ŒPaddleNLPå¯¹äºå„ç§é¢„è®­ç»ƒæ¨¡å‹å·²ç»å†…ç½®äº†å¯¹äºä¸‹æ¸¸ä»»åŠ¡æ–‡æœ¬åˆ†ç±»Fine-tuneç½‘ç»œã€‚ä»¥ä¸‹æ•™ç¨‹ä»¥ERNIEä¸ºé¢„è®­ç»ƒæ¨¡å‹å®Œæˆåºåˆ—æ ‡æ³¨ä»»åŠ¡ã€‚\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`ä¸€è¡Œä»£ç å³å¯åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ERNIEç”¨äºåºåˆ—æ ‡æ³¨ä»»åŠ¡çš„fine-tuneç½‘ç»œã€‚å…¶åœ¨ERNIEæ¨¡å‹åæ‹¼æ¥ä¸Šä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œè¿›è¡Œåˆ†ç±»ã€‚\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`æ–¹æ³•åªéœ€æŒ‡å®šæƒ³è¦ä½¿ç”¨çš„æ¨¡å‹åç§°å’Œæ–‡æœ¬åˆ†ç±»çš„ç±»åˆ«æ•°å³å¯å®Œæˆå®šä¹‰æ¨¡å‹ç½‘ç»œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model netword and its loss\r\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLPä¸ä»…æ”¯æŒERNIEé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿˜æ”¯æŒBERTã€RoBERTaã€Electraç­‰é¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "ä¸‹è¡¨æ±‡æ€»äº†ç›®å‰PaddleNLPæ”¯æŒçš„å„ç±»é¢„è®­ç»ƒæ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨PaddleNLPæä¾›çš„æ¨¡å‹ï¼Œå®Œæˆæ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€é—®ç­”ç­‰ä»»åŠ¡ã€‚åŒæ—¶æˆ‘ä»¬æä¾›äº†ä¼—å¤šé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°æƒé‡ä¾›ç”¨æˆ·ä½¿ç”¨ï¼Œå…¶ä¸­åŒ…å«äº†äºŒåå¤šç§ä¸­æ–‡è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡ã€‚ä¸­æ–‡çš„é¢„è®­ç»ƒæ¨¡å‹æœ‰`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`ç­‰ã€‚\n",
    "\n",
    "æ›´å¤šé¢„è®­ç»ƒæ¨¡å‹å‚è€ƒï¼š[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)ã€‚\n",
    "\n",
    "æ›´å¤šé¢„è®­ç»ƒæ¨¡å‹fine-tuneä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨æ–¹æ³•ï¼Œè¯·å‚è€ƒï¼š[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)ã€‚\n",
    "\n",
    "### 2.è¶…å‚è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\r\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=1.28e-4, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## äº”ã€æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "\n",
    "### 1.è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹é€šå¸¸æœ‰ä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. ä»dataloaderä¸­å–å‡ºä¸€ä¸ªbatch data\n",
    "2. å°†batch dataå–‚ç»™modelï¼Œåšå‰å‘è®¡ç®—\n",
    "3. å°†å‰å‘è®¡ç®—ç»“æœä¼ ç»™æŸå¤±å‡½æ•°ï¼Œè®¡ç®—lossã€‚å°†å‰å‘è®¡ç®—ç»“æœä¼ ç»™è¯„ä»·æ–¹æ³•ï¼Œè®¡ç®—è¯„ä»·æŒ‡æ ‡ã€‚\n",
    "4. lossåå‘å›ä¼ ï¼Œæ›´æ–°æ¢¯åº¦ã€‚é‡å¤ä»¥ä¸Šæ­¥éª¤ã€‚\n",
    "\n",
    "æ¯è®­ç»ƒä¸€ä¸ªepochæ—¶ï¼Œç¨‹åºå°†ä¼šè¯„ä¼°ä¸€æ¬¡ï¼Œè¯„ä¼°å½“å‰æ¨¡å‹è®­ç»ƒçš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step = 0\r\n",
    "for epoch in range(10):\r\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        step += 1\r\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\r\n",
    "    evaluate(model, metric, dev_loader)\r\n",
    "\r\n",
    "    paddle.save(model.state_dict(),\r\n",
    "                './checkpoint/model_%d.pdparams' % step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.æ¨¡å‹ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir ernie_result\r\n",
    "model.save_pretrained('./ernie_result')\r\n",
    "tokenizer.save_pretrained('./ernie_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## å…­ã€é¢„æµ‹\n",
    "\n",
    "###  1.é‡å¯notebookï¼Œé‡Šæ”¾æ˜¾å­˜\n",
    "\n",
    "é‡å¯notebookï¼Œé‡Šæ”¾æ˜¾å­˜ï¼Œå¼€å§‹è®­ç»ƒã€‚\n",
    "\n",
    "è®­ç»ƒä¿å­˜å¥½çš„è®­ç»ƒï¼Œå³å¯ç”¨äºé¢„æµ‹ã€‚å¦‚ä»¥ä¸‹ç¤ºä¾‹ä»£ç è‡ªå®šä¹‰é¢„æµ‹æ•°æ®ï¼Œè°ƒç”¨`predict()`å‡½æ•°å³å¯ä¸€é”®é¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import DataLoader\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from utils import convert_example, evaluate, predict, load_dict\r\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.å¯¼å…¥é¢„æµ‹æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n20 data/sample_per_line_preliminary_B.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.å®šä¹‰testæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            # next(fp)  # æ²¡æœ‰headerï¼Œä¸ç”¨Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                words = line.strip('\\n')\r\n",
    "                words=[ch for ch in words]\r\n",
    "                labels=['O' for x in range(0,len(words))]\r\n",
    "\r\n",
    "                yield words, labels\r\n",
    "                # yield words\r\n",
    "\r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ds = load_dataset(datafiles=('./data/sample_per_line_preliminary_B.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\r\n",
    "    print(test_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.åŠ è½½æœ€ä½³æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_vocab = load_dict('./mytag.dic')\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "test_ds.map(trans_func)\r\n",
    "print (test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = 1\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): fn(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_size=30,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_predict(model, data_loader, ds, label_vocab):\r\n",
    "    pred_list = []\r\n",
    "    len_list = []\r\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\r\n",
    "        logits = model(input_ids, seg_ids)\r\n",
    "        # print(len(logits[0]))\r\n",
    "        pred = paddle.argmax(logits, axis=-1)\r\n",
    "        pred_list.append(pred.numpy())\r\n",
    "        len_list.append(lens.numpy())\r\n",
    "    preds ,tags= parse_decodes(ds, pred_list, len_list, label_vocab)\r\n",
    "    return preds, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# é€‰æ‹©ç›®æ ‡checkpoint\r\n",
    "model_dict = paddle.load('checkpoint/model_2540.pdparams')\r\n",
    "model.set_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.é¢„æµ‹å¹¶ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\r\n",
    "preds, tags = my_predict(model, test_loader, test_ds, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tags[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"ernie_results.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "    fout.write(\"\\n\".join(preds))\r\n",
    "# Print some examples\r\n",
    "print(\r\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "    % file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head ernie_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# è°ƒæ•´æ•°æ®æ ¼å¼\r\n",
    "wp = open('data/sample_per_line_preliminary_B.txt', 'r', encoding='utf-8')\r\n",
    "lp = open('./ernie_results.txt', 'r', encoding='utf-8')\r\n",
    "with open('result.txt', 'w', encoding='utf-8') as f:\r\n",
    "    word_line = wp.readlines()\r\n",
    "    label_line = lp.readlines()\r\n",
    "    print('word_line:{},label_line {}'.format(len(word_line), len(label_line)))\r\n",
    "    for i in range(len(word_line)):\r\n",
    "        labels = label_line[i].strip('\\n').strip(' ').split(' ')\r\n",
    "        words = word_line[i].strip('\\n')\r\n",
    "        print('i:{} labels:{},words {}'.format(i, len(labels), len(words)))\r\n",
    "        for j in range(len(labels)):\r\n",
    "            f.write(words[j] + ' ' + labels[j] + '\\n')\r\n",
    "        f.write('\\n')\r\n",
    "wp.close()\r\n",
    "lp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
